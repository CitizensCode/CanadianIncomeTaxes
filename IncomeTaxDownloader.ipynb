{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "from urlparse import urlsplit, urljoin\n",
    "# Yeah I know I have scrapy above as well. I like them for different things.\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = 'http://www.cra-arc.gc.ca/formspubs/t1gnrl/llyrs-eng.html'\n",
    "base_page = urllib2.urlopen(base_url).read()\n",
    "selection = BeautifulSoup(base_page, 'html.parser')\n",
    "# print selection.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a url for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_items = selection.find(property=\"mainContentOfPage\").ul.find_all('li')\n",
    "year_pages_dict = {}\n",
    "year = 2014\n",
    "for item in year_items:\n",
    "    year_pages_dict[str(year)] = item.a.get(\"href\")\n",
    "    year = year - 1\n",
    "# year_pages_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_split = urlsplit(base_url)\n",
    "def completeURL(partial):\n",
    "    return 'http://' + url_split.netloc + partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_pages_dict_full = {}\n",
    "for year, url in year_pages_dict.iteritems():\n",
    "    # Create the url and then append it into the empty array\n",
    "    url_joined = completeURL(url)\n",
    "    year_pages_dict_full[year] = url_joined\n",
    "\n",
    "# Debugging\n",
    "# year_pages_dict_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "provincial_links = {}\n",
    "\n",
    "def provinceUrlListAppend(url_list, flag, year_url, year):\n",
    "    print year\n",
    "    if flag == \"normal\" or flag == \"older\":\n",
    "        for url in url_list:\n",
    "            provincial_links[year].append(completeURL(url))\n",
    "    elif flag == \"reallyOld\":\n",
    "        for url in url_list:\n",
    "            link = 'http://' + url_split.netloc\n",
    "            link += urlsplit(year_url).path.rsplit('/', 1)[0]\n",
    "            link += \"/\" + url\n",
    "            provincial_links[year].append(link)\n",
    "\n",
    "def provinceUrlListProcess(url_list, year_url, year):\n",
    "    provincial_links[year] = []\n",
    "    # If it's 'normal' and Alberta is detected, go ahead\n",
    "    if url_list[0] == 'Alberta' or url_list[0] == ' ':\n",
    "        url_list = selection.xpath(\"//main[@property='mainContentOfPage']/ul[1]/li/a/@href\").extract()\n",
    "        provinceUrlListAppend(url_list, \"normal\", year_url, year)\n",
    "    else:\n",
    "        # In some older links, they stuck another unordered list above\n",
    "        url_list = selection.xpath(\"//main[@property='mainContentOfPage']/ul[2]/li/a/@href\").extract()\n",
    "        # Another edge case check where they started using relative urls around 1998\n",
    "        if url_list[0][0] == \"/\":\n",
    "            provinceUrlListAppend(url_list, \"older\", year_url, year)\n",
    "        else:\n",
    "            provinceUrlListAppend(url_list, \"reallyOld\", year_url, year)\n",
    "\n",
    "for year, year_url in year_pages_dict_full.iteritems():\n",
    "    year_page = urllib2.urlopen(year_url).read()\n",
    "    selection = Selector(text=year_page)\n",
    "    provt_pages = selection.xpath(\"//main[@property='mainContentOfPage']/ul/li/a/text()\").extract()\n",
    "    provinceUrlListProcess(provt_pages, year_url, year)\n",
    "\n",
    "# print provincial_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadPdfProv(prov_url):\n",
    "    print prov_url\n",
    "    # Go to the provincial page, find the doc that has the tax brackets in it\n",
    "    page = urllib2.urlopen(prov_url).read()\n",
    "    page_text = BeautifulSoup(page, 'html.parser')\n",
    "    try: \n",
    "        download_url = completeURL(page_text.find('a', text = re.compile('428')).get(\"href\"))\n",
    "\n",
    "        # Go to the download page and find the pdf url\n",
    "        page = urllib2.urlopen(download_url).read()\n",
    "        page_text = BeautifulSoup(page, 'html.parser')\n",
    "        pdf_url = completeURL(page_text.find('a', text = re.compile('pdf')).get(\"href\"))\n",
    "        pdf_url\n",
    "\n",
    "        # Save the PDF locally\n",
    "        cwd = os.getcwd()\n",
    "        download_folder = os.path.join(cwd, \"IncomeTaxForms\")\n",
    "        file_name = os.path.join(download_folder, pdf_url.split(\"/\")[-1])\n",
    "\n",
    "        # Speeds things up if the file already exists\n",
    "        if not os.path.isfile(file_name):\n",
    "            urllib.urlretrieve(pdf_url, file_name)\n",
    "    except:\n",
    "        print \"FAILED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Works for all provinces 1999 - 2014, except Quebec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = \"1999\"\n",
    "year_links = provincial_links[year]\n",
    "for prov_url in year_links:\n",
    "    downloadPdfProv(prov_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Download federal 1999-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadPdfFed(year_url):\n",
    "    print year_url\n",
    "    # Go to the provincial page, find the doc that has the tax brackets in it\n",
    "    page = urllib2.urlopen(year_url).read()\n",
    "    page_text = BeautifulSoup(page, 'html.parser')\n",
    "    try: \n",
    "        download_url = completeURL(page_text.find('a', text = re.compile('Schedule 1')).get(\"href\"))\n",
    "\n",
    "        # Go to the download page and find the pdf url\n",
    "        page = urllib2.urlopen(download_url).read()\n",
    "        page_text = BeautifulSoup(page, 'html.parser')\n",
    "        pdf_url = completeURL(page_text.find('a', text = re.compile('pdf')).get(\"href\"))\n",
    "        pdf_url\n",
    "\n",
    "        # Save the PDF locally\n",
    "        cwd = os.getcwd()\n",
    "        download_folder = os.path.join(cwd, \"IncomeTaxForms\")\n",
    "        file_name = os.path.join(download_folder, pdf_url.split(\"/\")[-1])\n",
    "\n",
    "        # Speeds things up if the file already exists\n",
    "        if not os.path.isfile(file_name):\n",
    "            urllib.urlretrieve(pdf_url, file_name)\n",
    "    except:\n",
    "        print \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = xrange(1999, 2015)\n",
    "for year in years:\n",
    "    year_url = provincial_links[str(year)][0]\n",
    "    downloadPdfFed(year_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
